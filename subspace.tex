\documentclass{article}

\usepackage{fullpage,amssymb,amsthm,amsmath}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\R}{\mathbb{R}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\norm}[1]{\left\|#1\right\|}

\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\trace}{trace}

\begin{document}

\title{Subspace Clustering}
\author{D\'avid P\'al}
\maketitle

\section{Introduction}

Given a finite set of points $S \subset \R^d$ and a finite \textbf{set} of
vector subspaces $\V$ we define \emph{cost of $\V$ on $S$}
$$
f(S, \V) = \sum_{x \in S} \min_{V \in \V} \norm{x - \Pi_V x}_2^2
$$
where $\Pi_V$ is the $d\times d$ orthogonal projection matrix to a subspace $V$.
That is, if $v_1, v_2, \dots, v_r \in \R^d$ is an orthonormal basis
of a vector space $V$, then
$$
\Pi_V =
\begin{pmatrix}
\vert & \vert &  & \vert \\
v_1 & v_2 & \cdots & v_r \\
\vert & \vert &  & \vert \\
\end{pmatrix}
\begin{pmatrix}
- & v_1^T & - \\
- & v_2^T & - \\
  & \vdots &  \\
- & v_r^T & - \\
\end{pmatrix} \; .
$$
To see this notice that $v_i^T x$ is the projection of $x$ to $v_i$
and thus
$$
\begin{pmatrix}
- & v_1^T & - \\
- & v_2^T & - \\
  & \vdots & \\
- & v_r^T & - \\
\end{pmatrix} \cdot x
=
\begin{pmatrix}
v_1^T x \\
v_2^T x \\
\vdots  \\
v_r^T x \\
\end{pmatrix}
$$
is the vector of coordinates of the projection of $x$ expressed in the basis $v_1, v_2, \dots, v_r$.
Multiplying the vector of coordinates by
$$
\begin{pmatrix}
\vert & \vert &  & \vert \\
v_1 & v_2 & \cdots & v_r \\
\vert & \vert &  & \vert \\
\end{pmatrix}
$$
gives the projection of $x$ to $V$.

\subsection{Problem Statement}

Subspace clustering: \emph{Given a set of points $S$ and positive integers
$r_1, r_2, \dots, r_k$, find subspaces $V_1, V_2, \dots, V_k$ such that
$\dim(v_i) = d_i$ and $f(S, \{V_1, V_2, \dots, V_k\})$ is minimized.}

For $k=1$, the problem is equivalent to PCA. Recall that any $d \times d$
symmetric $A$ has an eigendecomposition
$$
A = U^T D U
$$
where $D$ is $d \times d$ diagonal matrix and $U$ is a $d \times d$
orthogonal matrix.

\begin{lemma}
Let $d$ be positive integers. Let $S$ be a finite subset of $\R^d$.
The subspace $V$ of dimension $1$ that minimizes $f(S, \{V\})$ is generated
by the first eigenvector of $\sum_{x \in S} xx^T$.
\end{lemma}

\begin{proof}
Let $A = \sum_{x \in S} xx^T$ be the $d \times d$
covariance matrix of the input points. Consider its eigendecomposition
$$
A = U^T D U
$$
where
$$
U =
\begin{pmatrix}
- & u_1^T & - \\
- & u_2^T & - \\
  & \vdots & \\
- & u_r^T & - \\
\end{pmatrix} \; .
$$
is $d \times d$ orthogonal matrix and $D = \diag(\lambda_1,
\lambda_2, \dots, \lambda_d)$ where $\lambda_1 \ge \lambda_2 \ge \dots \ge
\lambda_d \ge 0$ are the eigenvalues of $A$.

Consider any one-dimensional vector space $V$ generated by a unit vector $v$.
Then,
\begin{align*}
f(S, \{V\})
& = \sum_{x \in S} \norm{x - vv^T x}_2^2 \\
& = \sum_{x \in S} (x - vv^T x)^T (x - vv^Tx) \\
& = \sum_{x \in S} x^Tx + x^T vv^T vv^T x - 2 x^T vv^T x  \\
& = \sum_{x \in S} x^Tx + x^T vv^T x - 2 x^T vv^T x & \text{(since $v^Tv = 1$)} \\
& = \sum_{x \in S} x^Tx - x^T vv^T x  \\
& = \sum_{x \in S} x^Tx - v^T xx^T v  \\
& = \trace\left(\sum_{x \in S} xx^T \right) - v U^T D U v \\
& = \trace(U^T D U) - v U^T D U v \\
& = \trace(D) - (Uv)^T D (Uv) \; .
\end{align*}
Since $U$ is orthogonal, $z=Uv$ is a unit vector. Therefore,
$$
z^T D z = \sum_{i=1}^d \lambda_i z_i^2 \le \lambda_1 \sum_{i=1}^d z_i^2 = \lambda_1
$$
and the inequality holds with equality if and only if $z=(1,0,0,\dots,0)$.
That happens if and only if $v=u_1$ is the first eigenvector.
In other words, for any one-dimensional vector space $V$,
$$
f(S, \{V\}) \ge \trace(D) - \lambda_1
$$
and the inequality holds with inequality if and only if $V$ is generated
by the first eigenvector.
\end{proof}


\end{document}

\documentclass[12pt]{article}

\usepackage{fullpage,amssymb,amsthm,amsmath}
\usepackage{algorithm,algorithmic}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\R}{\mathbb{R}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\norm}[1]{\left\|#1\right\|}

\renewcommand{\v}{\mathbf{v}}
\newcommand{\x}{\mathbf{x}}
\renewcommand{\c}{\mathbf{c}}

\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\trace}{trace}
\DeclareMathOperator*{\Exp}{\mathbf{E}}

\begin{document}

\title{Subspace Clustering}
\author{D\'avid P\'al}
\maketitle

\section{Introduction}

In \emph{subspace clustering problem}, we are given a finite set of points $S =
\{\x_1, \x_2, \dots, \x_n\} \subset \R^d$ and positive integers $k,r$.  The
goal is to find vector subspaces $V_1, V_2, \dots, V_k \subseteq \R^d$, each of
dimension $r$, that minimize
\begin{equation}
\label{equation:cost-function}
\sum_{i=1}^n \min_{j=1,2,\dots,k} \left( d(\x_i, V_j) \right)^2
\end{equation}
where $d(\x,V)$ is the Euclidean distance between a point $\x$ and a subspace
$V$.

The distance between a subspace $V$ and a point $\x$ can be expressed as
$d(\x,V) = \norm{\x - \Pi_V \x}$ where $\norm{\cdot}$ is the Euclidean norm and
$\Pi_V$ is the $d \times d$ orthogonal projection matrix onto the subspace $V$.
That is, if $\v_1, \v_2, \dots, \v_r \in \R^d$ is an orthonormal basis of the
vector space $V$ then the projection matrix is
$$
\Pi_V =
\begin{pmatrix}
\vert & \vert &  & \vert \\
\v_1 & \v_2 & \cdots & \v_r \\
\vert & \vert &  & \vert \\
\end{pmatrix}
\begin{pmatrix}
- & \v_1^T & - \\
- & \v_2^T & - \\
  & \vdots &  \\
- & \v_r^T & - \\
\end{pmatrix} \; .
$$
To see this notice that $\v_i \v_i^T \x$ is the projection of $\x$ to $\v_i$
and thus
$$
\Pi_V \x
=
\begin{pmatrix}
\vert & \vert &  & \vert \\
\v_1 & \v_2 & \cdots & \v_r \\
\vert & \vert &  & \vert \\
\end{pmatrix}
\begin{pmatrix}
\v_1^T \\
\v_2^T \\
\vdots  \\
\v_r^T \\
\end{pmatrix}
\x
=
\begin{pmatrix}
\vert & \vert &  & \vert \\
\v_1 & \v_2 & \cdots & \v_r \\
\vert & \vert &  & \vert \\
\end{pmatrix}
\begin{pmatrix}
\v_1^T \x\\
\v_2^T \x\\
\vdots  \\
\v_r^T \x\\
\end{pmatrix}
= \sum_{i=1}^r \v_i \v_i^T \x \; .
$$

\section{Reduction to Sphere}
\label{section:sphere-reduction}

First, we consider a more general problem where each point has a weight
associated with it. Then, we show that, in this more general problem, we can
without a loss of generality assume that all the points lie on the unit sphere
(i.e. have unit norm).

In the \emph{weighted subspace clustering problem}, we are given a finite set
of weighted points $S = \{ (\x_1, w_1), (\x_2, w_2), \dots, (\x_n, w_n) \}
\subseteq \R^d \times \R_+$ and positive integers $k$ and $r$. The goal is to
find vector subspaces $V_1, V_2, \dots, V_k \subseteq \R^d$, each of dimension
$r$, that minimize
\begin{equation}
\label{equation:weighted-cost-function}
\sum_{i=1}^n w_i \min_{j=1,2,\dots,k} \left(d(\x_i, V_j)\right)^2 \; .
\end{equation}

Suppose we are given weighted points $(\x_1, w_1), (\x_2, w_2), \dots, (\x_n,
w_n)$. Consider weighted points $(\x'_1, w'_1), (\x'_2, w'_2), \dots, (\x'_n,
w'_n)$ defined as
$$
\x'_i = \frac{\x_i}{\norm{\x_i}} \qquad \text{and} \qquad
w'_i = w_i \norm{\x_i}^2 \; .
$$
Since $d(c \x, V) = |c| d(\x,V)$ for any $c \in \R$, the cost
\eqref{equation:weighted-cost-function} of any subspaces $V_1, V_2, \dots, V_k$
on $(\x'_1, w'_1), (\x'_2, w'_2), \dots, (\x'_n, w'_n)$ is
\begin{align*}
\sum_{i=1}^n w'_i \min_{j=1,2,\dots,k} \left(d(\x'_i, V_j)\right)^2
& = \sum_{i=1}^n w_i \norm{\x_i}^2 \min_{j=1,2,\dots,k} \left( \frac{d(\x_i, V_j)}{\norm{\x_i}} \right)^2 \\
& = \sum_{i=1}^n w_i \min_{j=1,2,\dots,k} \left( d(\x_i, V_j) \right)^2 \; ,
\end{align*}
which is the cost \eqref{equation:weighted-cost-function}
of $V_1, V_2, \dots, V_k$ on the original set of weighted points $S$.

\section{Line Clustering}
\label{section:line-clustering}

In this section, we consider the problem of \emph{weighted line clustering}
which is a special case of \emph{weighted subspace clustering} where the
dimension of the subspaces is $r=1$. We show an $O(\log k)$ approximation
algorithm for the problem. The algorithm uses \texttt{k-means++}
algorithm~\cite{Arthur-Vassilvitskii-2007} as a black box.

\subsection{\texttt{k-means++}}

Recall the \emph{$k$-means clustering problem}: We are given a set of points $S
= \{\x_1, \x_2, \dots, \x_n\} \subset \R^d$ and a positive integer $k$. The
goal is to find centers $\c_1, \c_2, \dots, \c_k \in \R^d$ that minimize
$$
\sum_{i=1}^n \min_{j=1,2,\dots,k} \norm{\x_i - \c_j}^2 \; .
$$
We consider \emph{weighted version} of the problem in which we are given a
finite set of weighted points $S = \{ (\x_1, w_1), (\x_2, w_2), \dots, (\x_n,
w_n)\} \subset \R^d \times \R_+$ and a positive integer $k$. The goal is to
find centers $\c_1, \c_2, \dots, \c_k \in \R^d$ that minimize
$$
\sum_{i=1}^n w_i \min_{j=1,2,\dots,k} \norm{\x_i - \c_j}^2 \; .
$$

The \texttt{k-means++} algorithm is a randomized algorithm for the (weighted)
$k$-means clustering problem. The centers chosen by the algorithm belong to the
original input set. The expected cost of solution is an $O(\log k)$
approximation of the optimal solution.

\begin{algorithm}[h]
\caption{Weighted \texttt{k-means++}
\label{algorithm:k-means++}}
\begin{algorithmic}
{
\REQUIRE{Finite set of weighted points $\{(\x_1, w_1), (\x_2, w_2), \dots, (\x_n, w_n) \} \subset \R^d$, number of centers $k \ge 1$.}
\STATE{Choose $\c_1$ at random according to distribution $\Pr[\c_1 = \x_i] = \frac{w_i}{\sum_{i=1}^n w_i}$.}
\FOR{$i=1,2,\dots,n$}
\STATE{$D[\x_i] \leftarrow +\infty$}
\ENDFOR
\FOR{$j=2,3,\dots,k$}
\FOR{$i=1,2,\dots,n$}
\STATE{$D[\x_i] \leftarrow \min\{D[\x_i], \norm{\x_i - \c_{j-1}}\}$}
\ENDFOR
\STATE{Choose $\c_j$ at random according to the distribution $\Pr[\c_j = \x_i] = \frac{w_i \cdot (D[\x_i])^2}{\sum_{\ell=1}^n w_\ell \cdot (D[\x_\ell])^2}$}
\ENDFOR
\RETURN{$\c_1, \c_2, \dots, \c_k$}
}
\end{algorithmic}
\end{algorithm}

The variable $D[\x_i]$ maintained by the algorithm is the distance from $\x_i$
to the closest center chosen so far. In other words, after iteration $j$ of the
outer for loop is $D[\x_i] = \min\{ \norm{\x_i - \c_1}, \norm{\x_i - \c_2},
\dots, \norm{x_i - \c_{j-1}}\}$.

\subsection{Reduction from Line Clustering to \texttt{k-means++}}

Suppose we are given a weighted set of points $S=\{(\x_1, w_1), (\x_2, w_2),
\dots, (\x_n, w_n)\} \subset \R^d \times \R_+$ and a positive integer $k$. We
would like to to find one-dimensional vector subspaces $V_1, V_2, \dots, V_k
\subseteq \R^d$ that minimize cost \eqref{equation:weighted-cost-function}. As
explained in Section~\ref{section:sphere-reduction}, we can assume without loss
of generality that $\norm{\x_1} = \norm{\x_2} = \dots = \norm{\x_n} = 1$.

To solve the problem, we define points $\widetilde \x_1, \widetilde \x_2,
\dots, \widetilde \x_n \in \R^{d^2}$. The coordinates of $\widetilde \x_i$ are
indexed by pairs $(p,q)$ where $p,q \in \{1,2,\dots,d\}$ and are defined as
\begin{equation}
\label{equation:outer-product-kernel}
\widetilde \x_i[p,q] = \x_i[p] \cdot \x_i[q] \; ,
\end{equation}
where $\x_i[p]$ and $\x_i[q]$ are the $p$-th and $q$-th coordinates of $\x_i$
respectively.

We give $\widetilde S = \{(\widetilde \x_1, w_1), (\widetilde \x_2, w_2), \dots,
(\widetilde \x_n, w_n) \}$ and $k$ as input to the weighted \texttt{k-means++}
algorithm (Algorithm~\ref{algorithm:k-means++}). The algorithm outputs centers
$\widetilde \c_1, \widetilde \c_2, \dots, \widetilde \c_k \in \R^{d^2}$ such
that $\{\widetilde \c_1, \widetilde \c_2, \dots, \widetilde \c_k\} \subseteq
\{\widetilde \x_1, \widetilde \x_2, \dots, \widetilde \x_n\}$. We can thus
identify each center $\widetilde \c_i \in \R^{d^2}$ with one of the original
points $\x_j \in \R^d$; let $\c_i$ be the point in $\R^d$ corresponding to
$\widetilde \c_i$. Let $V_i \subseteq \R^d$ be the one-dimensional subspace
generated by the vector $\c_i$.

As the next lemma shows, the weighted $k$-means cost of $\widetilde \c_1,
\widetilde \c_2, \dots, \widetilde \c_k$ on $\widetilde S$ is exactly twice the
weighted subspace clustering cost of $V_1, V_2, \dots, V_k$ on $S$.

\begin{lemma}
Let $\x, \c \in \R^d$ be arbitrary unit vectors. Let $\widetilde \x, \widetilde
\c \in \R^{d^2}$ be defined as $\widetilde \x[p,q] = \x[p] \cdot \x[q]$ and
$\widetilde \c[p,q] = \c[p] \cdot \c[q]$ for all $p,q \in \{1,2,\dots,d\}$. Let
$V$ be the one-dimensional vector space generated by $c$. Then,
$$
2 \left( d(\x,V) \right)^2 = \norm{\widetilde \x - \widetilde \c}^2 \; .
$$
\end{lemma}

\begin{proof}
We compute $\norm{\widetilde \x - \widetilde \c}^2$ as
\begin{align*}
\norm{\widetilde \x - \widetilde \c}^2
& = \norm{\x \x^T - \c \c^T}_F^2 \\
& = \trace\left( (\x\x^T - \c \c^T) (\x \x^T - \c \c^T)^T \right) \\
& = \trace\left( \x\x^T\x\x^T - \x\x^T \c\c^T - \c\c^T \x\x^T + \c \c^T \c \c^T \right) \\
& = \trace\left( \x\x^T\x\x^T \right) - \trace \left( \x\x^T \c\c^T \right) - \trace \left( \c\c^T \x\x^T \right) +  \trace\left(\c \c^T \c \c^T \right) \\
& = \trace\left( \underbrace{\x^T\x}_{=1} \underbrace{\x^T\x}_{=1} \right) - 2\trace \left( \x^T \c\c^T\x \right) +  \trace\left( \underbrace{\c^T \c}_{=1} \underbrace{\c^T \c}_{=1} \right) \\
& = 2 - 2(\c^T\x)^2 \; .
\end{align*}
We compute $(d(\x, V))^2$ as
\begin{align*}
d(\x, V)^2
& = \norm{\x - \Pi_{V} \x}^2 \\
& = \norm{\x - \c \c^T \x}^2 \\
& = \norm{\x}^2 - 2 \x^T \c \c^T \x + \norm{\c\c^T \x}^2 \\
& = \norm{\x}^2 - 2 (\c^T \x)^2  + (\c^T \x)^2 \\
& = 1 - (\c^T \x)^2 \; .
\end{align*}
Comparing the two expressions, we see they differ a by multiplicative factor of
two.
\end{proof}

Finally, note that the cost of the optimal weighted $k$-means clustering of
$\widetilde S$ is at most twice  the cost of the optimal weighted line
clustering of $S$. The reason is that any solution of $V_1, V_2, \dots, V_k$ to
the line clustering problem can be mapped to a solution on $\widetilde \c_1,
\widetilde \c_2, \dots, \widetilde \c_k \in \R^{d^2}$ with exactly twice the
cost. The mapping is as follows: Suppose that a subspace $V_i$ is generated by a
unit vector $\c_i \in \R^d$; we map $\c_i$ to $\widetilde \c_i \in \R^{d^2}$ via
the map $\widetilde \c_i[p,q] = \c_i[p] \cdot \c_i[q]$ where $p,q \in
\{1,2,\dots,d\}$ are coordinate indices.\footnote{Note the mapping is \emph{not}
onto, and hence not every point in $\R^{d^2}$ corresponds to a point in $\R^d$.
If we view $\R^{d^2}$ as a space of matrices, only symmetric rank-$1$ matrices
have a preimage.} Therefore, the resulting solution is an $O(\log k)$
approximation of optimal solution of the line clustering problem.

The algorithm can be implemented without explicitly constructing
any vectors in $\R^{d^2}$, working with the vectors in $\R^d$ only.

\section{Junk}

Subspace clustering: \emph{Given a set of points $S$ and positive integers
$r_1, r_2, \dots, r_k$, find subspaces $V_1, V_2, \dots, V_k$ such that
$\dim(v_i) = d_i$ and $f(S, \{V_1, V_2, \dots, V_k\})$ is minimized.}

For $k=1$, the problem is equivalent to PCA. Recall that any $d \times d$
symmetric $A$ has an eigendecomposition
$$
A = U^T D U
$$
where $D$ is $d \times d$ diagonal matrix and $U$ is a $d \times d$
orthogonal matrix.

\begin{lemma}
Let $S$ be a finite subset of $\R^d$. Let $V$ be a subspace
generated by a unit vector $v \in \R^d$. Then,
$$
f(S,\{V\})
= \left(\sum_{x \in S} \norm{x}_2^2 \right) - v^T A v
= \trace(A) - v^T A v
$$
where $A = \sum_{x \in S} xx^T$ is the covariance matrix of $S$.
\end{lemma}

\begin{proof}
We have
\begin{align*}
f(S, \{V\})
& = \sum_{x \in S} \norm{x - vv^T x}_2^2 \\
& = \sum_{x \in S} (x - vv^T x)^T (x - vv^Tx) \\
& = \sum_{x \in S} x^Tx + x^T vv^T vv^T x - 2 x^T vv^T x  \\
& = \sum_{x \in S} x^Tx + x^T vv^T x - 2 x^T vv^T x & \text{(since $v^Tv = 1$)} \\
& = \sum_{x \in S} x^Tx - x^T vv^T x  \\
& = \sum_{x \in S} x^Tx - v^T xx^T v  \\
& = \left(\sum_{x \in S} x^Tx \right) - v^T A v  \\
& = \left(\sum_{x \in S} \trace(xx^T) \right) - v^T A v \\
& = \trace \left(\sum_{x \in S} xx^T \right) - v^T A v \\
& = \trace(A) - v^T A v \; .
\end{align*}
\end{proof}

\begin{lemma}
Let $S$ be a finite subset of $\R^d$.
The subspace $V$ of dimension $1$ that minimizes $f(S, \{V\})$ is generated
by the first eigenvector of $\sum_{x \in S} xx^T$.
\end{lemma}

\begin{proof}
Let $A = \sum_{x \in S} xx^T$ be the $d \times d$
covariance matrix of the input points. Consider its eigendecomposition
$$
A = U^T D U
$$
where
$$
U =
\begin{pmatrix}
- & u_1^T & - \\
- & u_2^T & - \\
  & \vdots & \\
- & u_r^T & - \\
\end{pmatrix} \; .
$$
is $d \times d$ orthogonal matrix and $D = \diag(\lambda_1,
\lambda_2, \dots, \lambda_d)$ where $\lambda_1 \ge \lambda_2 \ge \dots \ge
\lambda_d \ge 0$ are the eigenvalues of $A$.

Consider any one-dimensional vector space $V$ generated by a unit vector $v$.
Then,
\begin{align*}
f(S, \{V\})
& = \trace(A) - v^T A v \\
& = \trace(U^T D U) - v U^T D U v \\
& = \trace(D) - (Uv)^T D (Uv) \; .
\end{align*}
Since $U$ is orthogonal, $z=Uv$ is a unit vector. Therefore,
$$
z^T D z = \sum_{i=1}^d \lambda_i z_i^2 \le \lambda_1 \sum_{i=1}^d z_i^2 = \lambda_1
$$
and the inequality holds with equality if and only if $z=(1,0,0,\dots,0)$.
That happens if and only if $v=u_1$ is the first eigenvector.
In other words, for any one-dimensional vector space $V$,
$$
f(S, \{V\}) \ge \trace(D) - \lambda_1
$$
and the inequality holds with inequality if and only if $V$ is generated
by the first eigenvector.
\end{proof}

\begin{lemma}[$D^2$ sampling]
Let $S$ be a finite subset of $\R^d$. Let $v$ be chosen at random from $S$
according to probability distribution $\Pr[v = u] = \frac{\norm{u}_2^2}{\sum_{z \in S} \norm{z}_2^2}$.
Let $V$ be the subspace generated by $v$. Then,
$$
\Exp[f(S,\{V\})] \le 2 \cdot \min_{V^*} f(S, \{V^*\})
$$
where the minimum is taken over all $1$-dimensional subspaces $V^*$.
\end{lemma}

\begin{proof}
Let $A = \sum_{x \in S} xx^T$ be the covariance matrix of the input points. Then,
$$
\Exp[f(S, \{V\})]
= \trace(A) - \Exp\left[ \frac{v^TAv}{\norm{v}_2^2} \right] \; .
$$
We compute $\Exp \left[\frac{v^TAv}{\norm{v}_2^2} \right]$ as
\begin{align*}
\Exp\left[ \frac{v^TAv}{\norm{v}_2^2} \right]
& = \frac{1}{\sum_{z \in S} \norm{z}_2^2} \sum_{v \in S} \norm{v}_2^2 \cdot \frac{v^T A v}{\norm{v}_2^2} \\
& = \frac{1}{\sum_{z \in S} \norm{z}_2^2} \sum_{v \in S} v^T A v \\
& = \frac{1}{\sum_{z \in S} \norm{z}_2^2} \sum_{v \in S} \sum_{x \in S} v^T x x^T v \\
& = \frac{1}{\sum_{z \in S} \norm{z}_2^2} \sum_{v \in S} \sum_{x \in S} (v^T x)^2
\end{align*}
In order to compute $\sum_{v,x} (v^T x)^2$, let $X$ be the $d \times |S|$
where each column corresponds to a point in $S$.
Let's consider the singular value decomposition of $X = U^T \Sigma V$
where $U$ is $d \times d$ orthogonal matrix, $V$ is $|S| \times |S|$
orthogonal matrix, and $\Sigma = \diag(\sigma_1, \sigma_2, \dots, \sigma_d)$ is $d \times |S|$ rectangular
diagonal matrix of singular values. Then,
\begin{align*}
\sum_{v \in S} \sum_{x \in S} (v^T x)^2
= \norm{X^T X}_F^2 = \trace(X^T X X^T X) = \trace(\Sigma^T \Sigma \Sigma^T \Sigma)
= \sum_{i=1}^d \sigma_i^4 \; .
\end{align*}
On the other hand,
$$
\sum_{z \in S} \norm{z}_2^2 = \norm{X}_F^2 = \trace\left(X^T X\right) = \trace \left( \Sigma^T \Sigma \right) = \sum_{i=1}^d \sigma_i^2 \; .
$$
Likewise, $\trace(A) = \sum_{i=1}^d \sigma_i^2$. Hence,
$$
\Exp[f(S, \{V\})]
= \trace(A) - \Exp\left[ \frac{v^TAv}{\norm{v}_2^2} \right]
= \sum_{i=1}^d \sigma_i^2 - \frac{\sum_{i=1}^d \sigma_i^4}{\sum_{i=1}^d \sigma_i^2} \; .
$$
Assuming that $\sigma_1^2 \le \sigma_2^2 \le \dots \le \sigma_d^2$, the optimal cost is
$$
\min_{V^*} f(S, \{V^*\}) = \sum_{i=1}^{d-1} \sigma_i^2 \; .
$$
It remains to verify that
$$
\sum_{i=1}^d \sigma_i^2 - \frac{\sum_{i=1}^d \sigma_i^4}{\sum_{i=1}^d \sigma_i^2} \le 2 \sum_{i=1}^{d-1} \sigma_i^2 \; .
$$
The inequality is equivalent to the inequalities below
\begin{align*}
\sigma_d^2 - \frac{\sum_{i=1}^d \sigma_i^4}{\sum_{i=1}^d \sigma_i^2} & \le \sum_{i=1}^{d-1} \sigma_i^2 \; , \\
\sigma_d^2 \sum_{i=1}^d \sigma_i^2 - \sum_{i=1}^d \sigma_i^4 & \le \left( \sum_{i=1}^{d-1} \sigma_i^2 \right) \left( \sum_{i=1}^d \sigma_i^2 \right) \; , \\
\sigma_d^4  + \sigma_d^2 \sum_{i=1}^{d-1} \sigma_i^2 - \sum_{i=1}^d \sigma_i^4 & \le \left( \sum_{i=1}^{d-1} \sigma_i^2 \right)^2 + \sigma_d^2 \left( \sum_{i=1}^{d-1} \sigma_i^2 \right) \; , \\
\sigma_d^4  - \sum_{i=1}^d \sigma_i^4 & \le \left( \sum_{i=1}^{d-1} \sigma_i^2 \right)^2 \; .
\end{align*}
\end{proof}

\section{Algorithm}

We consider an algorithm for one dimensional subspace clustering.

\begin{algorithm}[h]
\caption{Subspace clustering++
\label{algorithm:subspace-clustering++}}
\begin{algorithmic}
{
\REQUIRE{Finite set of points $S \subset \R^d$, number of subspaces $k \ge 1$.}
\FOR{$x \in S$}
\STATE{$D[x] \leftarrow \norm{x}$}
\ENDFOR
\FOR{$i=1,2,\dots,k$}
\STATE{Sample $v_i \in S$ according to the distribution $\Pr[v_i = x] = \frac{D[x]^2}{\sum_{z \in S} D[z]^2}$}
\STATE{$V_i = \langle v_i \rangle$}
\FOR{$x \in S$}
\STATE{$D[x] \leftarrow \min\{D[x], \ d(x, V_i)$\}}
\ENDFOR
\ENDFOR
\RETURN{$\{V_1, V_2, \dots, V_k\}$}
}
\end{algorithmic}
\end{algorithm}

\begin{lemma}
Let $S \subset \R^d$ be a finite set of points. Let $\V$ be a non-empty set of subspaces of dimension one.
Let $D(x) = \min_{V \in \V} d(x, V)$.  Suppose we choose $v \in S$ at random
according to $\Pr[v = x] = \frac{D(x)^2}{\sum_{z \in S} D(z)^2}$.
Then,
$$
\Exp\left[f(S, \V \cup \{\langle v \rangle\}) \right] \le 10 \cdot \min_u f(S, \{ V^* \})
$$
where the minimum is taken over all $1$-dimensional subspaces $V^*$.
\end{lemma}

\begin{proof}
\begin{align*}
\Exp\left[f(S, \V \cup \{\langle v \rangle\}) \right]
& = \sum_{v \in S} \frac{D(v)^2}{\sum_{z \in S} D(z)^2} \sum_{x \in S} \min \left\{D(x)^2, \ \norm{x - \frac{vv^Tx}{\norm{v}^2}}^2 \right\} \\
\end{align*}
TODO
\end{proof}

\section{$2D$ Case}

Without loss of generality, we can assume that $\norm{x} = 1$
for all $x \in S$. If $S \subset \R^2$, then we can think
of elements of $S$ as angles $\alpha_1, \alpha_2, \dots, \alpha_n$
in the interval $[0,2\pi)$. Let's assume that the

\begin{lemma}
For any $\alpha_1, \alpha_2, \dots, \alpha_n$ and any $\beta$,
$$
\frac{\sum_{i=1}^n \sum_{j=1}^n \sin^2 \alpha_j \cdot \min\{\sin^2 \alpha_i, \sin^2(\alpha_i - \alpha_j)\}}{\sum_{j=1}^n \sin^2 \alpha_j}
\le \sum_{i=1}^n \sin^2(\alpha_i - \beta) \; .
$$
\end{lemma}

\begin{proof}
The inequality is equivalent to
$$
\sum_{i=1}^n \sum_{j=1}^n \sin^2 \alpha_j \cdot \min\{\sin^2 \alpha_i, \sin^2(\alpha_i - \alpha_j)\}
\le
\left( \sum_{j=1}^n \sin^2 \alpha_j \right) \cdot \left( \sum_{i=1}^n \sin^2(\alpha_i - \beta) \right) \; .
$$
Using the identity $\sin(x-y) = \sin x \cos y - \cos x \sin y$, we have
$$
\sum_{i=1}^n \sum_{j=1}^n \sin^2 \alpha_j \cdot \min\{\sin^2 \alpha_i, (\sin \alpha_i \cos \alpha_j - \cos \alpha_i \sin \alpha_j)^2\}
\le
\left( \sum_{j=1}^n \sin^2 \alpha_j \right) \cdot \left( \sum_{i=1}^n (\sin \alpha_i \cos \beta - \cos \alpha_i \sin \beta)^2 \right) \; .
$$
This is equivalent to
\begin{multline*}
\sum_{i=1}^n \sum_{j=1}^n \sin^2 \alpha_j \cdot \min\{\sin^2 \alpha_i, \sin^2 \alpha_i \cos^2 \alpha_j + \cos^2 \alpha_i \sin^2 \alpha_j - 2 \sin \alpha_i \sin \alpha_j \cos \alpha_i \cos \alpha_j \}
\\ \le
\sum_{i=1}^n \sum_{j=1}^n \sin^2 \alpha_j \left(  \sin^2 \alpha_i \cos^2 \beta + \cos^2 \alpha_i \sin^2 \beta - 2 \sin \alpha_i \cos \alpha_i \sin \beta \cos \beta \right) \; .
\end{multline*}
\end{proof}

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}

\documentclass{article}

\usepackage{fullpage,amssymb,amsthm,amsmath}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\R}{\mathbb{R}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\norm}[1]{\left\|#1\right\|}

\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\trace}{trace}
\DeclareMathOperator*{\Exp}{\mathbf{E}}

\begin{document}

\title{Subspace Clustering}
\author{D\'avid P\'al}
\maketitle

\section{Introduction}

Given a finite set of points $S \subset \R^d$ and a finite \textbf{set} of
vector subspaces $\V$ we define \emph{cost of $\V$ on $S$}
$$
f(S, \V) = \sum_{x \in S} \min_{V \in \V} \norm{x - \Pi_V x}_2^2
$$
where $\Pi_V$ is the $d\times d$ orthogonal projection matrix to a subspace $V$.
That is, if $v_1, v_2, \dots, v_r \in \R^d$ is an orthonormal basis
of the vector space $V$, then
$$
\Pi_V =
\begin{pmatrix}
\vert & \vert &  & \vert \\
v_1 & v_2 & \cdots & v_r \\
\vert & \vert &  & \vert \\
\end{pmatrix}
\begin{pmatrix}
- & v_1^T & - \\
- & v_2^T & - \\
  & \vdots &  \\
- & v_r^T & - \\
\end{pmatrix} \; .
$$
To see this notice that $v_i^T x$ is the projection of $x$ to $v_i$
and thus
$$
\begin{pmatrix}
- & v_1^T & - \\
- & v_2^T & - \\
  & \vdots & \\
- & v_r^T & - \\
\end{pmatrix} \cdot x
=
\begin{pmatrix}
v_1^T x \\
v_2^T x \\
\vdots  \\
v_r^T x \\
\end{pmatrix}
$$
is the vector of coordinates of the projection of $x$ expressed in the basis $v_1, v_2, \dots, v_r$.
Multiplying the vector of coordinates by
$$
\begin{pmatrix}
\vert & \vert &  & \vert \\
v_1 & v_2 & \cdots & v_r \\
\vert & \vert &  & \vert \\
\end{pmatrix}
$$
gives the projection of $x$ to $V$.

\subsection{Problem Statement}

Subspace clustering: \emph{Given a set of points $S$ and positive integers
$r_1, r_2, \dots, r_k$, find subspaces $V_1, V_2, \dots, V_k$ such that
$\dim(v_i) = d_i$ and $f(S, \{V_1, V_2, \dots, V_k\})$ is minimized.}

For $k=1$, the problem is equivalent to PCA. Recall that any $d \times d$
symmetric $A$ has an eigendecomposition
$$
A = U^T D U
$$
where $D$ is $d \times d$ diagonal matrix and $U$ is a $d \times d$
orthogonal matrix.

\begin{lemma}
Let $S$ be a finite subset of $\R^d$. Let $V$ be a subspace
generated by a unit vector $v \in \R^d$. Then,
$$
f(S,\{V\})
= \left(\sum_{x \in S} \norm{x}_2^2 \right) - v^T A v
= \trace(A) - v^T A v
$$
where $A = \sum_{x \in S} xx^T$ is the covariance matrix of $S$.
\end{lemma}

\begin{proof}
We have
\begin{align*}
f(S, \{V\})
& = \sum_{x \in S} \norm{x - vv^T x}_2^2 \\
& = \sum_{x \in S} (x - vv^T x)^T (x - vv^Tx) \\
& = \sum_{x \in S} x^Tx + x^T vv^T vv^T x - 2 x^T vv^T x  \\
& = \sum_{x \in S} x^Tx + x^T vv^T x - 2 x^T vv^T x & \text{(since $v^Tv = 1$)} \\
& = \sum_{x \in S} x^Tx - x^T vv^T x  \\
& = \sum_{x \in S} x^Tx - v^T xx^T v  \\
& = \left(\sum_{x \in S} x^Tx \right) - v^T A v  \\
& = \left(\sum_{x \in S} \trace(xx^T) \right) - v^T A v \\
& = \trace \left(\sum_{x \in S} xx^T \right) - v^T A v \\
& = \trace(A) - v^T A v \; .
\end{align*}
\end{proof}

\begin{lemma}
Let $S$ be a finite subset of $\R^d$.
The subspace $V$ of dimension $1$ that minimizes $f(S, \{V\})$ is generated
by the first eigenvector of $\sum_{x \in S} xx^T$.
\end{lemma}

\begin{proof}
Let $A = \sum_{x \in S} xx^T$ be the $d \times d$
covariance matrix of the input points. Consider its eigendecomposition
$$
A = U^T D U
$$
where
$$
U =
\begin{pmatrix}
- & u_1^T & - \\
- & u_2^T & - \\
  & \vdots & \\
- & u_r^T & - \\
\end{pmatrix} \; .
$$
is $d \times d$ orthogonal matrix and $D = \diag(\lambda_1,
\lambda_2, \dots, \lambda_d)$ where $\lambda_1 \ge \lambda_2 \ge \dots \ge
\lambda_d \ge 0$ are the eigenvalues of $A$.

Consider any one-dimensional vector space $V$ generated by a unit vector $v$.
Then,
\begin{align*}
f(S, \{V\})
& = \trace(A) - v^T A v \\
& = \trace(U^T D U) - v U^T D U v \\
& = \trace(D) - (Uv)^T D (Uv) \; .
\end{align*}
Since $U$ is orthogonal, $z=Uv$ is a unit vector. Therefore,
$$
z^T D z = \sum_{i=1}^d \lambda_i z_i^2 \le \lambda_1 \sum_{i=1}^d z_i^2 = \lambda_1
$$
and the inequality holds with equality if and only if $z=(1,0,0,\dots,0)$.
That happens if and only if $v=u_1$ is the first eigenvector.
In other words, for any one-dimensional vector space $V$,
$$
f(S, \{V\}) \ge \trace(D) - \lambda_1
$$
and the inequality holds with inequality if and only if $V$ is generated
by the first eigenvector.
\end{proof}

\begin{lemma}[Uniform Sampling]
Let $S$ be a finite subset of $\R^d$. Let $v$ be chosen uniformly at random from $S$.
Let $V$ be the subspace generated by $v$. Then,
$$
\Exp[f(S,\{V\})] = ???
$$
\end{lemma}

\begin{proof}
Let $A = \sum_{x \in S} xx^T$ be the covariance matrix of the input points.
$$
\Exp[f(S, \{V\})]
= \trace(A) - \Exp\left[ \frac{v^TAv}{\norm{v}_2^2} \right] \; .
$$
It remains to compute $\Exp[\frac{v^TAv}{\norm{v}_2^2}]$. We have
\begin{align*}
\Exp\left[ \frac{v^TAv}{\norm{v}_2^2} \right]
& = \frac{1}{|S|} \sum_{v \in S} \frac{v^TAv}{\norm{v}_2^2} \\
& = \frac{1}{|S|} \sum_{v \in S} \sum_{x \in S} \frac{v^T x x^T v}{\norm{v}_2^2} \\
& = TODO
\end{align*}

\end{proof}

\begin{lemma}[$D^2$ sampling]
Let $S$ be a finite subset of $\R^d$. Let $v$ be chosen at random from $S$
according to probability distribution $\Pr[v = u] = \frac{\norm{u}_2^2}{\sum_{z \in S} \norm{z}_2^2}$.
Let $V$ be the subspace generated by $v$. Then,
$$
\Exp[f(S,\{V\})] = ???
$$
\end{lemma}

\begin{proof}
Let $A = \sum_{x \in S} xx^T$ be the covariance matrix of the input points.
$$
\Exp[f(S, \{V\})]
= \trace(A) - \Exp\left[ \frac{v^TAv}{\norm{v}_2^2} \right] \; .
$$
It remains to compute $\Exp[\frac{v^TAv}{\norm{v}_2^2}]$. We have
\begin{align*}
\Exp\left[ \frac{v^TAv}{\norm{v}_2^2} \right]
& = \frac{1}{\sum_{z \in S} \norm{z}_2^2} \sum_{v \in S} \norm{v}_2^2 \cdot \frac{v^T A v}{\norm{v}_2^2} \\
& = \frac{1}{\sum_{z \in S} \norm{z}_2^2} \sum_{v \in S} v^T A v \\
& = \frac{1}{\sum_{z \in S} \norm{z}_2^2} \sum_{v \in S} \sum_{x \in S} v^T x x^T v \\
& = \frac{1}{\sum_{z \in S} \norm{z}_2^2} \sum_{v \in S} \sum_{x \in S} (v^T x)^2 \\
\end{align*}
In order to compute $\sum_{v,x} (v^T x)^2$, let $X$ be the $d \times |S|$
where each column corresponds to a point in $S$.
Let's consider the singular value decomposition of $X = U^T \Sigma V$
where $U$ is $d \times d$ orthogonal matrix, $V$ is $|S| \times |S|$
orthogonal matrix, and $\Sigma = \diag(\sigma_1, \sigma_2, \dots, \sigma_d)$ is $d \times |S|$ rectangular
diagonal matrix of singular values. Then,
\begin{align*}
\sum_{v \in S} \sum_{x \in S} (v^T x)^2
= \norm{X^T X}_F^2 = \trace(X^T X X^T X) = \trace(\Sigma^T \Sigma \Sigma^T \Sigma) 
= \sum_{i=1}^d \sigma_i^4 \; .
\end{align*}
TODO
\end{proof}

\end{document}

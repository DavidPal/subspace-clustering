\documentclass{article}

\usepackage{fullpage,amssymb,amsthm,amsmath}
\usepackage{algorithm,algorithmic}

\newtheorem{theorem}{Theorem}
\newtheorem{lemma}[theorem]{Lemma}

\newcommand{\R}{\mathbb{R}}
\newcommand{\V}{\mathcal{V}}
\newcommand{\norm}[1]{\left\|#1\right\|}

\DeclareMathOperator*{\diag}{diag}
\DeclareMathOperator*{\trace}{trace}
\DeclareMathOperator*{\Exp}{\mathbf{E}}

\begin{document}

\title{Subspace Clustering}
\author{D\'avid P\'al}
\maketitle

\section{Introduction}

In \emph{subspace clustering problem}, we are given
a finite set of points $S = \{x_1, x_2, \dots, x_n\} \subset \R^d$,
a positive integers $k,r$
The goal is to find $k$ vector subspaces $V_1, V_2, \dots, V_k \subseteq \R^d$,
each of dimension $r$, that minimize
\begin{equation}
\label{equation:cost-function}
\sum_{i=1}^n \min_{j=1,2,\dots,k} \left( d(x_i, V_j) \right)^2
\end{equation}
where $d(x,V)$ is the Euclidean distance between a point $x$
and a subspace $V$.

The distance between a subspace $V$ and a point $x$ can be expressed as
$d(x,V) = \norm{x - \Pi_V x}$
where $\norm{\cdot}$ is the Euclidean norm and $\Pi_V$ is
the $d \times d$ orthogonal projection matrix onto the subspace $V$.
That is, if $v_1, v_2, \dots, v_r \in \R^d$ is an orthonormal basis
of the vector space $V$ then the projection matrix is
$$
\Pi_V =
\begin{pmatrix}
\vert & \vert &  & \vert \\
v_1 & v_2 & \cdots & v_r \\
\vert & \vert &  & \vert \\
\end{pmatrix}
\begin{pmatrix}
- & v_1^T & - \\
- & v_2^T & - \\
  & \vdots &  \\
- & v_r^T & - \\
\end{pmatrix} \; .
$$
To see this notice that $v_i v_i^T x$ is the projection of $x$ to $v_i$
and thus
$$
\Pi_V x
=
\begin{pmatrix}
\vert & \vert &  & \vert \\
v_1 & v_2 & \cdots & v_r \\
\vert & \vert &  & \vert \\
\end{pmatrix}
\begin{pmatrix}
v_1^T \\
v_2^T \\
\vdots  \\
v_r^T \\
\end{pmatrix}
x
=
\begin{pmatrix}
\vert & \vert &  & \vert \\
v_1 & v_2 & \cdots & v_r \\
\vert & \vert &  & \vert \\
\end{pmatrix}
\begin{pmatrix}
v_1^T x\\
v_2^T x\\
\vdots  \\
v_r^T x\\
\end{pmatrix}
= \sum_{i=1}^r v_i v_i^T x \; .
$$

\section{Reduction to Sphere}

First, we consider a more general problem where each point has a weight
associated with it. We then show that in this more general problem we can
without a loss of generality assume that all the points lie on the unit sphere
(i.e. have unit norm).

In the \emph{weighted subspace clustering problem},
we are given a set of weighted points $S = \{ (x_1, w_1), (x_2,w_2), \dots, (x_n, w_n) \} \subseteq \R^d \times \R_+$
and positive integers $k$ and $r$. The goal is to find vector subspaces $V_1, V_2, \dots, V_k \subseteq \R^d$,
each of dimension $r$, that minimize
\begin{equation}
\label{equation:weighted-cost-function}
\sum_{i=1}^n w_i \min_{j=1,2,\dots,k} \left(d(x_i, V_j)\right)^2 \; .
\end{equation}

Suppose we are given any weighted points $(x_1, w_1), (x_2, w_2), \dots, (x_n, w_n)$.
Consider weighted points $(x'_1, w'_1), (x'_2, w'_2), \dots, (x'_n, w'_n)$ where
\begin{align*}
x'_i & = \frac{x_i}{\norm{x_i}} & \text{and} &&
w'_i & = w_i \norm{x_i}^2 \; .
\end{align*}
Since $d(c x, V) = |c| d(x,V)$ for any $c \in \R$,
the cost \label{equation:weighted-cost-function} applied
to $(x'_1, w'_1), (x'_2, w'_2), \dots, (x'_n, w'_n)$ is
$$
\sum_{i=1}^n w'_i \min_{j=1,2,\dots,k} \left(d(x'_i, V_j)\right)^2
= \sum_{i=1}^n w_i \norm{x_i}^2 \min_{j=1,2,\dots,k} \left( \norm{x_i} d(x_i, V_j) \right)^2
= \sum_{i=1}^n w_i \min_{j=1,2,\dots,k} \left( d(x_i, V_j) \right)^2 \; .
$$

\section{Line Clustering}

In this section, we consider the problem of \emph{weighted line clustering}
which is special case of subspace clustering with \emph{weighted subspace
clustering} where the dimension of the subspaces is $r=1$.
We show an $O(\log k)$ approximation algorithm for the problem.
The algorithm relies on $k$-means++ algorithm of Aurthur and Vassilivistki.

\subsection{Junk}

Subspace clustering: \emph{Given a set of points $S$ and positive integers
$r_1, r_2, \dots, r_k$, find subspaces $V_1, V_2, \dots, V_k$ such that
$\dim(v_i) = d_i$ and $f(S, \{V_1, V_2, \dots, V_k\})$ is minimized.}

For $k=1$, the problem is equivalent to PCA. Recall that any $d \times d$
symmetric $A$ has an eigendecomposition
$$
A = U^T D U
$$
where $D$ is $d \times d$ diagonal matrix and $U$ is a $d \times d$
orthogonal matrix.

\begin{lemma}
Let $S$ be a finite subset of $\R^d$. Let $V$ be a subspace
generated by a unit vector $v \in \R^d$. Then,
$$
f(S,\{V\})
= \left(\sum_{x \in S} \norm{x}_2^2 \right) - v^T A v
= \trace(A) - v^T A v
$$
where $A = \sum_{x \in S} xx^T$ is the covariance matrix of $S$.
\end{lemma}

\begin{proof}
We have
\begin{align*}
f(S, \{V\})
& = \sum_{x \in S} \norm{x - vv^T x}_2^2 \\
& = \sum_{x \in S} (x - vv^T x)^T (x - vv^Tx) \\
& = \sum_{x \in S} x^Tx + x^T vv^T vv^T x - 2 x^T vv^T x  \\
& = \sum_{x \in S} x^Tx + x^T vv^T x - 2 x^T vv^T x & \text{(since $v^Tv = 1$)} \\
& = \sum_{x \in S} x^Tx - x^T vv^T x  \\
& = \sum_{x \in S} x^Tx - v^T xx^T v  \\
& = \left(\sum_{x \in S} x^Tx \right) - v^T A v  \\
& = \left(\sum_{x \in S} \trace(xx^T) \right) - v^T A v \\
& = \trace \left(\sum_{x \in S} xx^T \right) - v^T A v \\
& = \trace(A) - v^T A v \; .
\end{align*}
\end{proof}

\begin{lemma}
Let $S$ be a finite subset of $\R^d$.
The subspace $V$ of dimension $1$ that minimizes $f(S, \{V\})$ is generated
by the first eigenvector of $\sum_{x \in S} xx^T$.
\end{lemma}

\begin{proof}
Let $A = \sum_{x \in S} xx^T$ be the $d \times d$
covariance matrix of the input points. Consider its eigendecomposition
$$
A = U^T D U
$$
where
$$
U =
\begin{pmatrix}
- & u_1^T & - \\
- & u_2^T & - \\
  & \vdots & \\
- & u_r^T & - \\
\end{pmatrix} \; .
$$
is $d \times d$ orthogonal matrix and $D = \diag(\lambda_1,
\lambda_2, \dots, \lambda_d)$ where $\lambda_1 \ge \lambda_2 \ge \dots \ge
\lambda_d \ge 0$ are the eigenvalues of $A$.

Consider any one-dimensional vector space $V$ generated by a unit vector $v$.
Then,
\begin{align*}
f(S, \{V\})
& = \trace(A) - v^T A v \\
& = \trace(U^T D U) - v U^T D U v \\
& = \trace(D) - (Uv)^T D (Uv) \; .
\end{align*}
Since $U$ is orthogonal, $z=Uv$ is a unit vector. Therefore,
$$
z^T D z = \sum_{i=1}^d \lambda_i z_i^2 \le \lambda_1 \sum_{i=1}^d z_i^2 = \lambda_1
$$
and the inequality holds with equality if and only if $z=(1,0,0,\dots,0)$.
That happens if and only if $v=u_1$ is the first eigenvector.
In other words, for any one-dimensional vector space $V$,
$$
f(S, \{V\}) \ge \trace(D) - \lambda_1
$$
and the inequality holds with inequality if and only if $V$ is generated
by the first eigenvector.
\end{proof}

\begin{lemma}[$D^2$ sampling]
Let $S$ be a finite subset of $\R^d$. Let $v$ be chosen at random from $S$
according to probability distribution $\Pr[v = u] = \frac{\norm{u}_2^2}{\sum_{z \in S} \norm{z}_2^2}$.
Let $V$ be the subspace generated by $v$. Then,
$$
\Exp[f(S,\{V\})] \le 2 \cdot \min_{V^*} f(S, \{V^*\})
$$
where the minimum is taken over all $1$-dimensional subspaces $V^*$.
\end{lemma}

\begin{proof}
Let $A = \sum_{x \in S} xx^T$ be the covariance matrix of the input points. Then,
$$
\Exp[f(S, \{V\})]
= \trace(A) - \Exp\left[ \frac{v^TAv}{\norm{v}_2^2} \right] \; .
$$
We compute $\Exp \left[\frac{v^TAv}{\norm{v}_2^2} \right]$ as
\begin{align*}
\Exp\left[ \frac{v^TAv}{\norm{v}_2^2} \right]
& = \frac{1}{\sum_{z \in S} \norm{z}_2^2} \sum_{v \in S} \norm{v}_2^2 \cdot \frac{v^T A v}{\norm{v}_2^2} \\
& = \frac{1}{\sum_{z \in S} \norm{z}_2^2} \sum_{v \in S} v^T A v \\
& = \frac{1}{\sum_{z \in S} \norm{z}_2^2} \sum_{v \in S} \sum_{x \in S} v^T x x^T v \\
& = \frac{1}{\sum_{z \in S} \norm{z}_2^2} \sum_{v \in S} \sum_{x \in S} (v^T x)^2
\end{align*}
In order to compute $\sum_{v,x} (v^T x)^2$, let $X$ be the $d \times |S|$
where each column corresponds to a point in $S$.
Let's consider the singular value decomposition of $X = U^T \Sigma V$
where $U$ is $d \times d$ orthogonal matrix, $V$ is $|S| \times |S|$
orthogonal matrix, and $\Sigma = \diag(\sigma_1, \sigma_2, \dots, \sigma_d)$ is $d \times |S|$ rectangular
diagonal matrix of singular values. Then,
\begin{align*}
\sum_{v \in S} \sum_{x \in S} (v^T x)^2
= \norm{X^T X}_F^2 = \trace(X^T X X^T X) = \trace(\Sigma^T \Sigma \Sigma^T \Sigma)
= \sum_{i=1}^d \sigma_i^4 \; .
\end{align*}
On the other hand,
$$
\sum_{z \in S} \norm{z}_2^2 = \norm{X}_F^2 = \trace\left(X^T X\right) = \trace \left( \Sigma^T \Sigma \right) = \sum_{i=1}^d \sigma_i^2 \; .
$$
Likewise, $\trace(A) = \sum_{i=1}^d \sigma_i^2$. Hence,
$$
\Exp[f(S, \{V\})]
= \trace(A) - \Exp\left[ \frac{v^TAv}{\norm{v}_2^2} \right]
= \sum_{i=1}^d \sigma_i^2 - \frac{\sum_{i=1}^d \sigma_i^4}{\sum_{i=1}^d \sigma_i^2} \; .
$$
Assuming that $\sigma_1^2 \le \sigma_2^2 \le \dots \le \sigma_d^2$, the optimal cost is
$$
\min_{V^*} f(S, \{V^*\}) = \sum_{i=1}^{d-1} \sigma_i^2 \; .
$$
It remains to verify that
$$
\sum_{i=1}^d \sigma_i^2 - \frac{\sum_{i=1}^d \sigma_i^4}{\sum_{i=1}^d \sigma_i^2} \le 2 \sum_{i=1}^{d-1} \sigma_i^2 \; .
$$
The inequality is equivalent to the inequalities below
\begin{align*}
\sigma_d^2 - \frac{\sum_{i=1}^d \sigma_i^4}{\sum_{i=1}^d \sigma_i^2} & \le \sum_{i=1}^{d-1} \sigma_i^2 \; , \\
\sigma_d^2 \sum_{i=1}^d \sigma_i^2 - \sum_{i=1}^d \sigma_i^4 & \le \left( \sum_{i=1}^{d-1} \sigma_i^2 \right) \left( \sum_{i=1}^d \sigma_i^2 \right) \; , \\
\sigma_d^4  + \sigma_d^2 \sum_{i=1}^{d-1} \sigma_i^2 - \sum_{i=1}^d \sigma_i^4 & \le \left( \sum_{i=1}^{d-1} \sigma_i^2 \right)^2 + \sigma_d^2 \left( \sum_{i=1}^{d-1} \sigma_i^2 \right) \; , \\
\sigma_d^4  - \sum_{i=1}^d \sigma_i^4 & \le \left( \sum_{i=1}^{d-1} \sigma_i^2 \right)^2 \; .
\end{align*}
\end{proof}

\section{Algorithm}

We consider an algorithm for one dimensional subspace clustering.

\begin{algorithm}[h]
\caption{Subspace clustering++
\label{algorithm:subspace-clustering++}}
\begin{algorithmic}
{
\REQUIRE{Finite set of points $S \subset \R^d$, number of subspaces $k \ge 1$.}
\FOR{$x \in S$}
\STATE{$D[x] \leftarrow \norm{x}$}
\ENDFOR
\FOR{$i=1,2,\dots,k$}
\STATE{Sample $v_i \in S$ according to the distribution $\Pr[v_i = x] = \frac{D[x]^2}{\sum_{z \in S} D[z]^2}$}
\STATE{$V_i = \langle v_i \rangle$}
\FOR{$x \in S$}
\STATE{$D[x] \leftarrow \min\{D[x], \ d(x, V_i)$\}}
\ENDFOR
\ENDFOR
\RETURN{$\{V_1, V_2, \dots, V_k\}$}
}
\end{algorithmic}
\end{algorithm}

\begin{lemma}
Let $S \subset \R^d$ be a finite set of points. Let $\V$ be a non-empty set of subspaces of dimension one.
Let $D(x) = \min_{V \in \V} d(x, V)$.  Suppose we choose $v \in S$ at random
according to $\Pr[v = x] = \frac{D(x)^2}{\sum_{z \in S} D(z)^2}$.
Then,
$$
\Exp\left[f(S, \V \cup \{\langle v \rangle\}) \right] \le 10 \cdot \min_u f(S, \{ V^* \})
$$
where the minimum is taken over all $1$-dimensional subspaces $V^*$.
\end{lemma}

\begin{proof}
\begin{align*}
\Exp\left[f(S, \V \cup \{\langle v \rangle\}) \right]
& = \sum_{v \in S} \frac{D(v)^2}{\sum_{z \in S} D(z)^2} \sum_{x \in S} \min \left\{D(x)^2, \ \norm{x - \frac{vv^Tx}{\norm{v}^2}}^2 \right\} \\
\end{align*}
TODO
\end{proof}

\section{$2D$ Case}

Without loss of generality, we can assume that $\norm{x} = 1$
for all $x \in S$. If $S \subset \R^2$, then we can think
of elements of $S$ as angles $\alpha_1, \alpha_2, \dots, \alpha_n$
in the interval $[0,2\pi)$. Let's assume that the

\begin{lemma}
For any $\alpha_1, \alpha_2, \dots, \alpha_n$ and any $\beta$,
$$
\frac{\sum_{i=1}^n \sum_{j=1}^n \sin^2 \alpha_j \cdot \min\{\sin^2 \alpha_i, \sin^2(\alpha_i - \alpha_j)\}}{\sum_{j=1}^n \sin^2 \alpha_j}
\le \sum_{i=1}^n \sin^2(\alpha_i - \beta) \; .
$$
\end{lemma}

\begin{proof}
The inequality is equivalent to
$$
\sum_{i=1}^n \sum_{j=1}^n \sin^2 \alpha_j \cdot \min\{\sin^2 \alpha_i, \sin^2(\alpha_i - \alpha_j)\}
\le
\left( \sum_{j=1}^n \sin^2 \alpha_j \right) \cdot \left( \sum_{i=1}^n \sin^2(\alpha_i - \beta) \right) \; .
$$
Using the identity $\sin(x-y) = \sin x \cos y - \cos x \sin y$, we have
$$
\sum_{i=1}^n \sum_{j=1}^n \sin^2 \alpha_j \cdot \min\{\sin^2 \alpha_i, (\sin \alpha_i \cos \alpha_j - \cos \alpha_i \sin \alpha_j)^2\}
\le
\left( \sum_{j=1}^n \sin^2 \alpha_j \right) \cdot \left( \sum_{i=1}^n (\sin \alpha_i \cos \beta - \cos \alpha_i \sin \beta)^2 \right) \; .
$$
This is equivalent to
\begin{multline*}
\sum_{i=1}^n \sum_{j=1}^n \sin^2 \alpha_j \cdot \min\{\sin^2 \alpha_i, \sin^2 \alpha_i \cos^2 \alpha_j + \cos^2 \alpha_i \sin^2 \alpha_j - 2 \sin \alpha_i \sin \alpha_j \cos \alpha_i \cos \alpha_j \}
\\ \le
\sum_{i=1}^n \sum_{j=1}^n \sin^2 \alpha_j \left(  \sin^2 \alpha_i \cos^2 \beta + \cos^2 \alpha_i \sin^2 \beta - 2 \sin \alpha_i \cos \alpha_i \sin \beta \cos \beta \right) \; .
\end{multline*}
\end{proof}

\bibliographystyle{plain}
\bibliography{biblio}

\end{document}
